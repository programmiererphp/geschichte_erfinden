
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Interactive Story App</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">

  <style>
    *, *::before, *::after { box-sizing: border-box; }
    body { font-family: 'Roboto', Arial, sans-serif; margin: 0; padding: 20px; background-color: #eef2f7; color: #333; line-height: 1.6; }
    .container { max-width: 800px; margin: 20px auto; background: #ffffff; padding: 25px 30px; border-radius: 12px; box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1); }
    h1, h2, h3 { text-align: center; color: #2c3e50; margin-top: 0; margin-bottom: 15px; }
    h1 { font-size: 2em; margin-bottom: 25px; }
    h2 { font-size: 1.4em; border-bottom: 2px solid #dfe6e9; padding-bottom: 8px; margin-bottom: 15px; }
    h3 { font-size: 1.2em; color: #34495e; }
    button { padding: 10px 18px; margin: 5px; font-size: 0.95em; cursor: pointer; border: none; border-radius: 6px; background-color: #3498db; color: white; transition: background-color 0.2s ease, transform 0.1s ease; font-weight: bold; }
    button:hover:not(:disabled) { background-color: #2980b9; transform: translateY(-1px); box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
    button:active:not(:disabled) { transform: translateY(0px); }
    button:disabled { background-color: #bdc3c7; cursor: not-allowed; opacity: 0.7; }
    .section { border: 1px solid #dfe6e9; padding: 15px; background: #f8f9fa; margin: 15px 0; border-radius: 8px; min-height: 40px; }
    #controlsContainer { display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-bottom: 20px; }
    #stopTTSBtn { background-color: #e74c3c; margin-left: 15px; }
    #stopTTSBtn:hover:not(:disabled) { background-color: #c0392b; }
    #stopTTSBtn:disabled { background-color: #e74c3c; opacity: 0.6; }
    #recordButtons { text-align: center; margin-top: 15px; padding-top: 15px; border-top: 1px dashed #bdc3c7; }
    #recordButtons button { font-size: 1.1em; padding: 12px 25px; margin: 0 10px; }
    #startRecordBtn { background-color: #2ecc71; }
    #startRecordBtn:hover:not(:disabled) { background-color: #27ae60; }
    #stopRecordBtn { background-color: #f39c12; }
    #stopRecordBtn:hover:not(:disabled) { background-color: #e67e22; }
    #recordCountdown { font-weight: bold; text-align: center; margin: 15px 0 10px 0; color: #e74c3c; }
    #loadingIndicator { display: none; text-align: center; padding: 15px; color: #555; font-style: italic; font-weight: bold; background-color: #ecf0f1; border-radius: 5px; margin: 15px 0; }
    #apiCountdown { font-weight: normal; margin-left: 10px; color: #3498db; }
    .debugDetail { font-size: 0.8em; color: #555; margin-top: 10px; border-top: 1px dashed #bdc3c7; padding-top: 8px; white-space: pre-wrap; word-wrap: break-word; background-color: #fdfdfe; padding: 8px; border-radius: 4px; }
    .modal { display: none; position: fixed; z-index: 10; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0,0,0,0.6); }
    .modal-content { background-color: #fff; margin: 5% auto; padding: 25px 30px; border: 1px solid #ccc; width: 90%; max-width: 750px; border-radius: 10px; box-shadow: 0 5px 20px rgba(0,0,0,0.2); }
    .close { color: #aaa; float: right; font-size: 32px; font-weight: bold; cursor: pointer; line-height: 1; }
    .close:hover, .close:focus { color: #333; text-decoration: none; }
    textarea, input[type="text"], input[type="time"], select { width: 100%; padding: 10px; margin: 5px 0 18px 0; box-sizing: border-box; border: 1px solid #ccc; border-radius: 6px; background-color: #fdfdfd; font-family: inherit; font-size: 0.95em; }
    textarea { min-height: 80px; }
    label { display: block; margin-bottom: 5px; font-weight: bold; color: #555; }
    .modelInfo { font-size: 0.9em; color: #777; margin-top: 15px; text-align: center; }
    #debugLog { height: 250px; overflow-y: auto; border: 1px solid #ccc; background: #f4f4f9; padding: 10px; font-family: monospace; font-size: 0.9em; white-space: pre-wrap; word-wrap: break-word; border-radius: 6px; }
    .theme-instructions { display: none; margin-top: 10px; padding: 15px; border: 1px dashed #3498db; background-color: #eaf5ff; border-radius: 6px; }
    .theme-instructions textarea { background-color: #fff; font-size: 0.85em; color: #555; }
    .warning { color: #e74c3c; font-size: 0.85em; margin-top: -10px; margin-bottom: 10px; }
  </style>
</head>
<body>
  <div class="container">
    <h1>Interactive Story App</h1>

    <div id="controlsContainer">
        <div>
            <button id="startGameBtn">Start Game</button>
            <button id="restartGameBtn">Restart Game</button>
            <button id="stopGameBtn">Stop Game</button>
            <button id="settingsBtn">Settings & Debug</button>
            <button id="showContribsBtn">Show Contributions</button>
        </div>
        <button id="stopTTSBtn" disabled>Stop Speaking</button>
    </div>

    <div id="modelInfo" class="modelInfo">
      Using models: STT: whisper-1 | TTS: Browser SpeechSynthesis
    </div>

    <div id="loadingIndicator">
      Processing request... Please wait.
      <span id="apiCountdown"></span>
    </div>

    <div id="recordCountdown"></div>

    <!-- Output Sections -->
    <div id="benutzerBeitrag" class="section">
      <h2>Last User Input (Transcript)</h2>
      <div id="lastUserText">No input yet.</div>
    </div>
    <div id="ironicCommentSection" class="section">
      <h2>Comment / Feedback</h2>
      <div id="ironicText">Waiting for the first contribution...</div>
    </div>
    <div id="currentStory" class="section">
      <h2>Current Story</h2>
      <div id="storyText">No story started yet.</div>
      <div id="storyDebug" class="debugDetail" style="display:none;">No Story Debug Info.</div>
    </div>
    <div id="nextHint" class="section">
      <h2>Hint</h2>
      <div id="hintText">Start the game to get a hint.</div>
       <div id="recordButtons" style="display:none;">
         <button id="startRecordBtn">Start Recording</button>
         <button id="stopRecordBtn" disabled>Stop Recording</button>
       </div>
      <div id="hintDebug" class="debugDetail" style="display:none;">No Hint Debug Info.</div>
    </div>
  </div>

  <!-- Settings Modal -->
  <div id="settingsModal" class="modal">
    <div class="modal-content">
      <span class="close" id="settingsClose">×</span>
      <h2>Settings & Debug</h2>
      <div>
        <label for="apiKey">OpenAI API Key:</label>
        <input type="text" id="apiKey" placeholder="Enter API key here (sk-...)">
      </div>
      <div>
        <label for="languageSelect">Interface & TTS Language:</label>
        <select id="languageSelect">
          <option value="de">Deutsch</option>
          <option value="en">English</option>
          <option value="ru">Русский</option>
        </select>
      </div>
      <hr>
      <h3>Text-to-Speech (TTS) Settings</h3>
      <div>
        <label for="ttsProviderSelect">TTS Provider:</label>
        <select id="ttsProviderSelect">
            <option value="browser">Browser Default</option>
            <option value="elevenlabs">ElevenLabs</option>
            <option value="google">Google Cloud TTS</option>
        </select>
      </div>
      <div id="elevenlabsSettings">
          <label for="elevenLabsKey">ElevenLabs API Key:</label>
          <input type="text" id="elevenLabsKey" placeholder="ElevenLabs API Key (optional)">
          <label for="elevenLabsVoice">ElevenLabs Voice ID:</label>
          <input type="text" id="elevenLabsVoice" placeholder="Voice ID (e.g., TxGEqnHWrfWFTfGW9XjX)">
      </div>
       <div id="googleTtsSettings">
           <label for="googleApiKey">Google Cloud API Key:</label>
           <input type="text" id="googleApiKey" placeholder="Google Cloud API Key (optional)">
           <div class="warning">Note: Using Google API Key client-side is insecure for production. Restrict your key!</div>
           <label for="googleVoiceName">Google Cloud Voice Name:</label>
           <input type="text" id="googleVoiceName" placeholder="e.g., en-US-Wavenet-D, de-DE-Standard-F">
           <a href="https://cloud.google.com/text-to-speech/docs/voices" target="_blank" rel="noopener noreferrer">Find Voice Names</a> |
           <a href="https://console.cloud.google.com/apis/credentials" target="_blank" rel="noopener noreferrer">Get API Key</a>
       </div>
      <div>
        <label for="ttsStyle">TTS Style Prompt (for AI Voice Generation - affects some models):</label>
        <textarea id="ttsStyle" rows="2">
Please speak like a cheerful, enthusiastic storyteller reading a bedtime story with a warm and engaging voice.
        </textarea>
      </div>
      <hr>
       <h3>Story Generation Settings</h3>
      <div>
        <label for="initialPrompt">Initial Prompt (Story Start):</label>
        <textarea id="initialPrompt" rows="6">
Give me a fascinating starting sentence for a story and also a short hint on how to continue the story so far (consider the previous story), returned as a question that begins with a short question about the continuation and contains exactly three answer options in parentheses (2 serious, one absurdly ironic), e.g.:
"How does it continue? (e.g.: 'He calls for caution', 'He sings a calming song', 'He dances a crazy salsa')"

Respond exclusively with a valid JSON object containing exactly the keys "startSentence" and "hint". The entire output MUST be valid JSON.
        </textarea>
      </div>
      <div>
        <label for="reactionPrompt">Reaction Prompt (Integration & Comment):</label>
        <textarea id="reactionPrompt" rows="9">
Story Request: You have this story so far: "{story}"
and the last user input: "{lastUser}".
Integrate the user contribution fluently into the story and provide a short, dryly ironic comment on the last user input, relating the comment to the last user input.
Also, provide a new, short hint on how to continue the story (consider the new complete story with the user input), returned as a question starting with a short question about the continuation and containing exactly three answer options in parentheses (two serious, one absurdly ironic), e.g.:
"How does it continue? (e.g.: 'He calls for caution', 'He sings a calming song', 'He dances a crazy salsa')"

Respond exclusively with a valid JSON object containing exactly the keys "integratedStory", "comment", and "hint". The entire output MUST be valid JSON.
        </textarea>
      </div>
       <!-- Theme Instructions -->
       <div id="germanA2PromptSection" class="theme-instructions">
         <label>Additional Instruction for "Learn German A2":</label>
         <textarea id="germanA2Prompt" rows="12" readonly>
# Special instructions for the theme "Learn German A2":
1.  **Story Language Level (`integratedStory`, `hint`):** Keep the generated story and hint in **simple German (A2 level)**. Use short sentences, common vocabulary, and basic grammar structures (Präsens, Perfekt, simple subordinate clauses).
2.  **Error Analysis:** Analyze the user input (`{lastUser}`) for typical A2 level German grammar mistakes (e.g., wrong articles, verb conjugation, sentence structure, case errors after prepositions).
3.  **Comment Language (`comment`):** Generate the comment **in {commentLang}**. The comment should be short, slightly ironic, but *helpful*, directly addressing *one* identified grammar mistake. Briefly explain the error and provide the correct form. Example (if commentLang is English): "Haha, almost! Remember, after 'mit' (with) we always use Dativ, so it's 'mit dem Freund' (with the friend - masc.), not 'mit den Freund'. But the idea of a friend in the fridge is... interesting!" If no typical A2 error is found, provide a generally encouraging comment in {commentLang}.
4.  **Hint Focus (`hint`):** The hint (in German A2) should subtly encourage practicing the grammar topic that was incorrect in the user input. E.g., if Dativ after 'mit' was wrong, the hint could be: "Was passiert als Nächstes? (z.B.: 'Er spricht mit [wem? Dativ!]...', 'Sie geht ohne [wen? Akkusativ!]...', 'Die Katze springt auf [wohin? Akkusativ!]...')". Focus on *one* grammar topic per hint.
5.  **JSON Structure:** Strictly maintain the JSON structure `{"integratedStory": "...", "comment": "...", "hint": "..."}`.
        </textarea>
      </div>
      <div id="lifecoachPromptSection" class="theme-instructions">
         <label>Additional Instruction for "Lifecoach":</label>
         <textarea id="lifecoachPrompt" rows="12" readonly>
# Special instructions for the theme "Lifecoach (Playful & Spontaneous)":
1.  **Persona:** Adopt the persona of a *playful, spontaneous, helpful, and easy-going lifecoach* within the narrative.
2.  **Story Integration (`integratedStory`):** Weave the story forward, potentially modeling the desired traits (playfulness, spontaneity) or presenting lighthearted scenarios related to the user's input. The tone should be encouraging and light.
3.  **Comment/Feedback (`comment`):** Provide a *helpful and encouraging comment* in {commentLang}, acting as the lifecoach. Relate it to the user's input or the story's situation. Offer a simple reflection, a playful observation, or a gentle nudge towards spontaneity or ease. Example: "Great idea to just jump in! Sometimes the best plan is no plan, right? What tiny spontaneous thing could happen next?"
4.  **Hint (`hint`):** The hint should encourage playful or spontaneous next steps, or pose a simple reflective question related to ease or helpfulness. Example: "What happens now? (e.g. 'They decide to sing to the ducks', 'They take an unexpected detour just for fun', 'They offer a stranger a compliment')"
5.  **JSON Structure:** Strictly maintain the JSON structure `{"integratedStory": "...", "comment": "...", "hint": "..."}`.
        </textarea>
      </div>
      <div id="creativewritingPromptSection" class="theme-instructions">
         <label>Additional Instruction for "Creative Writing Assistant":</label>
         <textarea id="creativewritingPrompt" rows="12" readonly>
# Special instructions for the theme "Creative Writing Assistant":
1.  **Focus:** Prioritize evocative language, sensory details, and creative descriptions. The plot might advance slowly.
2.  **Story Integration (`integratedStory`):** Continue the story, focusing on expanding the description of the scene, characters' feelings, or atmosphere based on the user's input. Use metaphors, similes, or interesting vocabulary.
3.  **Comment/Feedback (`comment`):** Provide feedback in {commentLang} on the user's contribution from a writing perspective. It could praise a good description, gently suggest adding more sensory detail, or offer a related mini-writing prompt. Example: "Nice action! How could you describe the *sound* of that happening? Adding sounds can really bring a scene to life."
4.  **Hint (`hint`):** The hint should prompt the user to focus on a specific writing technique or descriptive element. Example: "How does the scene continue? (e.g. 'Describe the smell in the air', 'Focus on the main character's internal thought', 'Use a metaphor to describe the light')"
5.  **JSON Structure:** Strictly maintain the JSON structure `{"integratedStory": "...", "comment": "...", "hint": "..."}`.
        </textarea>
      </div>
      <div id="mindfulnessPromptSection" class="theme-instructions">
         <label>Additional Instruction for "Mindfulness Moment":</label>
         <textarea id="mindfulnessPrompt" rows="12" readonly>
# Special instructions for the theme "Mindfulness Moment":
1.  **Pacing:** The story should be calm and progress slowly. Focus on presence and awareness.
2.  **Story Integration (`integratedStory`):** Gently guide the narrative towards a moment of awareness or simple observation related to the user's input. Describe a sensory detail or a simple action mindfully.
3.  **Comment/Feedback (`comment`):** Offer a short, calming reflection in {commentLang} related to the story or input. It could be a gentle reminder to breathe or notice something simple. Example: "A quiet moment. Take a breath... what do you notice around the character right now? Even small details matter."
4.  **Hint (`hint`):** The hint should guide the user towards a mindful action or observation within the story. Example: "What happens next? (e.g. 'They pause and listen to the sounds around them', 'They focus on the feeling of the ground beneath their feet', 'They simply watch a leaf fall')"
5.  **JSON Structure:** Strictly maintain the JSON structure `{"integratedStory": "...", "comment": "...", "hint": "..."}`.
        </textarea>
      </div>
      <div id="historicalwhatifPromptSection" class="theme-instructions">
         <label>Additional Instruction for "Historical What-If":</label>
         <textarea id="historicalwhatifPrompt" rows="12" readonly>
# Special instructions for the theme "Historical What-If":
1.  **Context:** Assume the story starts with or involves a specific historical setting or event.
2.  **Story Integration (`integratedStory`):** Explore the immediate consequences of the user's input within the established historical context. How does the timeline diverge?
3.  **Comment/Feedback (`comment`):** Provide a comment in {commentLang} that reflects (perhaps ironically) on the historical plausibility or the potential ramifications of the user's alternate history choice. Example: "Interesting turn! If that *had* happened in 1688, the price of tea might be very different today, wouldn't it?"
4.  **Hint (`hint`):** The hint should prompt the user to consider the next logical (or perhaps illogical but fun) consequence in this alternate timeline. Example: "What is the immediate result? (e.g. 'A rival faction reacts to this change', 'A new technology emerges because of this', 'Pigeons are suddenly declared the ruling class')"
5.  **JSON Structure:** Strictly maintain the JSON structure `{"integratedStory": "...", "comment": "...", "hint": "..."}`.
        </textarea>
      </div>
      <!-- End Theme Instructions -->
      <div>
        <label for="themeSelect">Theme (optional):</label>
        <select id="themeSelect">
          <option value="standard">Standard</option>
          <option value="romantik">Romance</option>
          <option value="nachhaltig">Sustainable</option>
          <option value="physik">Physics</option>
          <option value="lustig">Funny</option>
          <option value="scifi">Sci-Fi</option>
          <option value="fantasy">Fantasy</option>
          <option value="german-a2">Learn German A2</option>
          <option value="lifecoach">Lifecoach (Playful)</option>
          <option value="creative-writing">Creative Writing Assistant</option>
          <option value="mindfulness">Mindfulness Moment</option>
          <option value="historical-whatif">Historical What-If</option>
        </select>
      </div>
      <hr>
      <h3>Other Settings</h3>
      <div>
        <label for="autoStartTime">Auto-Start Time (HH:MM):</label>
        <input type="time" id="autoStartTime" value="17:00">
      </div>
      <div>
        <label><input type="checkbox" id="useBrowserSTT"> Use browser's speech recognition (no API key needed)</label>
      </div>
      <div>
        <label><input type="checkbox" id="showHelpInfos"> Show debug info for API requests</label>
      </div>
      <div id="modelInfoSettings" class="modelInfo">
        Using models: STT: whisper-1 | TTS: Browser SpeechSynthesis
      </div>
      <h3>Debug Log</h3>
      <div id="debugLog"></div>
      <button id="copyDebugBtn">Copy Debug Log</button>
      <button id="clearDebugBtn">Clear Debug Log</button>
    </div>
  </div>

  <!-- Contributions Modal -->
  <div id="contributionsModal" class="modal">
    <div class="modal-content">
      <span class="close" id="contribsClose">×</span>
      <h2>All User Contributions</h2>
      <ul id="contribList"></ul>
    </div>
  </div>

  <script>
    /* Global Variables */
    let apiKey = localStorage.getItem("openai_api_key") || "";
    let googleApiKey = localStorage.getItem("google_api_key") || ""; // New
    let gameActive = false;
    let fullStory = ""; let currentHint = ""; let lastComment = ""; let lastUserInput = "";
    let mediaRecorder; let audioChunks = []; let recordCountdownInterval;
    const RECORD_TIME = 20; let userContributions = [];
    let currentLanguage = localStorage.getItem("story_language") || "de";
    let currentTtsProvider = localStorage.getItem("tts_provider") || "browser"; // New
    let useBrowserSTT = localStorage.getItem("use_browser_stt") === "true" || !localStorage.getItem("openai_api_key"); // Use browser STT if enabled or no API key
    let recognitionInstance = null; // Speech recognition instance

    const languageNameMap = { de: "Deutsch", en: "English", ru: "Russisch" };
    const googleLangCodeMap = { de: 'de-DE', en: 'en-US', ru: 'ru-RU' }; // New

    let isProcessing = false; let isSpeaking = false;
    let currentAudioElement = null; let apiCountdownInterval;
    const ESTIMATED_API_TIME = 25;

    let currentSTTModel = "whisper-1"; let currentTTSModel = "Browser TTS"; // Will be updated

    let lastStartRequest = "", lastStartResponse = ""; let lastReactionRequest = "", lastReactionResponse = "";

    // --- DOM Element Caching ---
    // (Assuming IDs match HTML, simplified for brevity)
    const startGameBtn = document.getElementById("startGameBtn");
    const restartGameBtn = document.getElementById("restartGameBtn");
    const stopGameBtn = document.getElementById("stopGameBtn");
    const settingsBtn = document.getElementById("settingsBtn");
    const showContribsBtn = document.getElementById("showContribsBtn");
    const stopTTSBtn = document.getElementById("stopTTSBtn");
    const modelInfoDiv = document.getElementById("modelInfo");
    const loadingIndicator = document.getElementById("loadingIndicator");
    const apiCountdownSpan = document.getElementById("apiCountdown");
    const recordCountdownDiv = document.getElementById("recordCountdown");
    const recordButtonsDiv = document.getElementById("recordButtons");
    const startRecordBtn = document.getElementById("startRecordBtn");
    const stopRecordBtn = document.getElementById("stopRecordBtn");
    const useBrowserSTTCheckbox = document.getElementById("useBrowserSTT");
    const lastUserInputTitle = document.querySelector("#benutzerBeitrag h2");
    const lastUserTextDiv = document.getElementById("lastUserText");
    const commentTitle = document.querySelector("#ironicCommentSection h2");
    const ironicTextDiv = document.getElementById("ironicText");
    const storyTitle = document.querySelector("#currentStory h2");
    const storyTextDiv = document.getElementById("storyText");
    const hintTitle = document.querySelector("#nextHint h2");
    const hintTextDiv = document.getElementById("hintText");
    // Modals
    const settingsModal = document.getElementById("settingsModal");
    const settingsClose = document.getElementById("settingsClose");
    const contributionsModal = document.getElementById("contributionsModal");
    const contribsClose = document.getElementById("contribsClose");
    const contributionsTitle = document.querySelector("#contributionsModal h2");
    const contribList = document.getElementById("contribList");
    // Settings Inputs
    const apiKeyInput = document.getElementById("apiKey");
    const languageSelect = document.getElementById("languageSelect");
    const initialPromptInput = document.getElementById("initialPrompt");
    const reactionPromptInput = document.getElementById("reactionPrompt");
    // Theme Prompts
    const germanA2PromptSection = document.getElementById("germanA2PromptSection");
    const germanA2Prompt = document.getElementById("germanA2Prompt");
    const lifecoachPromptSection = document.getElementById("lifecoachPromptSection");
    const lifecoachPrompt = document.getElementById("lifecoachPrompt");
    const creativewritingPromptSection = document.getElementById("creativewritingPromptSection");
    const creativewritingPrompt = document.getElementById("creativewritingPrompt");
    const mindfulnessPromptSection = document.getElementById("mindfulnessPromptSection");
    const mindfulnessPrompt = document.getElementById("mindfulnessPrompt");
    const historicalwhatifPromptSection = document.getElementById("historicalwhatifPromptSection");
    const historicalwhatifPrompt = document.getElementById("historicalwhatifPrompt");
    const themeSelect = document.getElementById("themeSelect");
    // TTS Settings
    const ttsProviderSelect = document.getElementById("ttsProviderSelect"); // New
    const elevenlabsSettingsDiv = document.getElementById("elevenlabsSettings"); // New
    const elevenLabsKeyInput = document.getElementById("elevenLabsKey");
    const elevenLabsVoiceInput = document.getElementById("elevenLabsVoice");
    const googleTtsSettingsDiv = document.getElementById("googleTtsSettings"); // New
    const googleApiKeyInput = document.getElementById("googleApiKey"); // New
    const googleVoiceNameInput = document.getElementById("googleVoiceName"); // New
    const ttsStyleInput = document.getElementById("ttsStyle");
    // Other Settings
    const autoStartTimeInput = document.getElementById("autoStartTime");
    const showHelpInfosCheckbox = document.getElementById("showHelpInfos");
    const modelInfoSettingsDiv = document.getElementById("modelInfoSettings");
    const debugLogDiv = document.getElementById("debugLog");
    const copyDebugBtn = document.getElementById("copyDebugBtn");
    const clearDebugBtn = document.getElementById("clearDebugBtn");
    const storyDebugDiv = document.getElementById("storyDebug");
    const hintDebugDiv = document.getElementById("hintDebug");

    // --- Initialization & Event Listeners ---

    document.addEventListener('DOMContentLoaded', () => {
        apiKeyInput.value = apiKey;
        googleApiKeyInput.value = googleApiKey; // Load Google Key
        languageSelect.value = currentLanguage;
        ttsProviderSelect.value = currentTtsProvider; // Load TTS Provider pref
        elevenLabsKeyInput.value = localStorage.getItem("elevenlabs_key") || "";
        elevenLabsVoiceInput.value = localStorage.getItem("elevenlabs_voice") || "";
        googleVoiceNameInput.value = localStorage.getItem("google_voice_name") || ""; // Load Google Voice pref
        useBrowserSTTCheckbox.checked = useBrowserSTT; // Set browser STT checkbox
        handleTtsProviderChange(); // Show relevant TTS settings
        handleThemeChange(); // Show relevant theme prompt
        debugLog("Application initialized.");
        updateUIState();
        updateModelInfo(); // Update based on loaded provider pref
    });

    // --- Event Listeners Setup ---
    function setupEventListeners() {
        settingsBtn.addEventListener("click", () => openModal("settingsModal"));
        settingsClose.addEventListener("click", () => closeModal("settingsModal"));
        showContribsBtn.addEventListener("click", () => { updateContribsModal(); openModal("contributionsModal"); });
        contribsClose.addEventListener("click", () => closeModal("contributionsModal"));
        startGameBtn.addEventListener("click", () => { if (isProcessing || isSpeaking) { alert("Please wait..."); return; } debugLog("Start Game clicked."); startGame(); });
        restartGameBtn.addEventListener("click", () => { if (isProcessing || isSpeaking) { alert("Please wait..."); return; } debugLog("Restart Game clicked."); restartGame(); });
        stopGameBtn.addEventListener("click", () => { if (isProcessing) { alert("Please wait..."); return; } debugLog("Stop Game clicked."); stopGame(); });
        startRecordBtn.addEventListener("click", () => { if (isProcessing || isSpeaking) { alert("Please wait..."); return; } debugLog("Start Rec clicked."); recordUserSpeech(); });
        stopRecordBtn.addEventListener("click", () => { debugLog("Stop Rec clicked."); stopRecording(); });
        stopTTSBtn.addEventListener("click", () => { debugLog("Stop TTS clicked."); stopSpeaking(); });

        // Settings listeners
        apiKeyInput.addEventListener("change", () => { 
            apiKey = apiKeyInput.value.trim(); 
            localStorage.setItem("openai_api_key", apiKey); 
            useBrowserSTT = !apiKey; // Auto use browser STT if no API key
            useBrowserSTTCheckbox.checked = useBrowserSTT;
            localStorage.setItem("use_browser_stt", useBrowserSTT);
            updateModelInfo();
            debugLog("OpenAI Key " + (apiKey ? "saved." : "removed.") + (useBrowserSTT ? " Using browser STT." : "")); 
        });
        googleApiKeyInput.addEventListener("change", () => { googleApiKey = googleApiKeyInput.value.trim(); localStorage.setItem("google_api_key", googleApiKey); debugLog("Google API Key " + (googleApiKey ? "saved." : "removed.")); });
        languageSelect.addEventListener("change", () => { currentLanguage = languageSelect.value; localStorage.setItem("story_language", currentLanguage); handleThemeChange(); debugLog(`Lang changed to: ${currentLanguage}`); });
        themeSelect.addEventListener("change", handleThemeChange);
        ttsProviderSelect.addEventListener("change", handleTtsProviderChange); // Listener for provider change
        elevenLabsKeyInput.addEventListener("change", () => { localStorage.setItem("elevenlabs_key", elevenLabsKeyInput.value.trim()); debugLog("ElevenLabs Key saved."); });
        elevenLabsVoiceInput.addEventListener("change", () => { localStorage.setItem("elevenlabs_voice", elevenLabsVoiceInput.value.trim()); debugLog("ElevenLabs Voice saved."); });
        googleVoiceNameInput.addEventListener("change", () => { localStorage.setItem("google_voice_name", googleVoiceNameInput.value.trim()); debugLog("Google Voice saved."); });
        useBrowserSTTCheckbox.addEventListener("change", () => { 
            useBrowserSTT = useBrowserSTTCheckbox.checked; 
            localStorage.setItem("use_browser_stt", useBrowserSTT); 
            updateModelInfo();
            debugLog("Browser STT " + (useBrowserSTT ? "enabled." : "disabled.")); 
        });

        // Debug listeners
        copyDebugBtn.addEventListener("click", () => { navigator.clipboard.writeText(debugLogDiv.innerText).then(() => { debugLog("Log copied."); alert("Log copied!"); }).catch(err => { debugLog(`Copy error: ${err}`); alert("Copy failed."); }); });
        clearDebugBtn.addEventListener("click", () => { debugLogDiv.innerHTML = ""; debugLog("Log cleared."); });
        showHelpInfosCheckbox.addEventListener("change", updateHelpInfosVisibility);
    }
    setupEventListeners();

     // Show/Hide TTS Provider settings
     function handleTtsProviderChange() {
        currentTtsProvider = ttsProviderSelect.value;
        localStorage.setItem("tts_provider", currentTtsProvider);
        debugLog(`TTS provider set to: ${currentTtsProvider}`);
        elevenlabsSettingsDiv.style.display = (currentTtsProvider === 'elevenlabs') ? 'block' : 'none';
        googleTtsSettingsDiv.style.display = (currentTtsProvider === 'google') ? 'block' : 'none';
        updateModelInfo(); // Update displayed TTS model name
     }

     // Theme change handler (now includes updating A2 prompt placeholder)
     function handleThemeChange() {
        const sections = [germanA2PromptSection, lifecoachPromptSection, creativewritingPromptSection, mindfulnessPromptSection, historicalwhatifPromptSection];
        sections.forEach(sec => sec.style.display = 'none');
        const selectedTheme = themeSelect.value;
        let relevantSection = null;
        let relevantPromptTextarea = null;
        switch(selectedTheme) {
            case 'german-a2': relevantSection = germanA2PromptSection; relevantPromptTextarea = germanA2Prompt; break;
            case 'lifecoach': relevantSection = lifecoachPromptSection; relevantPromptTextarea = lifecoachPrompt; break;
            case 'creative-writing': relevantSection = creativewritingPromptSection; relevantPromptTextarea = creativewritingPrompt; break;
            case 'mindfulness': relevantSection = mindfulnessPromptSection; relevantPromptTextarea = mindfulnessPrompt; break;
            case 'historical-whatif': relevantSection = historicalwhatifPromptSection; relevantPromptTextarea = historicalwhatifPrompt; break;
        }
        if(relevantSection) {
             relevantSection.style.display = 'block';
             const langName = languageNameMap[currentLanguage] || "the selected language";
             if(relevantPromptTextarea) { relevantPromptTextarea.value = relevantPromptTextarea.value.replace(/in \{commentLang\}/g, `in ${langName}`); }
        }
         debugLog(`Theme changed to: ${selectedTheme}.`);
     }

    // --- Core Logic Functions ---

    function updateUIState() { /* ... (UI state logic as before, using English text directly) ... */
        if (gameActive) {
            storyTextDiv.textContent = fullStory || "No story started yet."; hintTextDiv.textContent = currentHint || "Start the game to get a hint.";
            lastUserTextDiv.textContent = lastUserInput || "Waiting for the first contribution..."; ironicTextDiv.textContent = lastComment || "Waiting for the first contribution...";
        } else {
            storyTextDiv.textContent = "No story started yet."; hintTextDiv.textContent = "Start the game to get a hint.";
            lastUserTextDiv.textContent = "No input yet."; ironicTextDiv.textContent = "Game stopped."; recordCountdownDiv.textContent = "";
        }
        const canRecord = gameActive && !isProcessing && !isSpeaking;
        startGameBtn.disabled = gameActive || isProcessing || isSpeaking; restartGameBtn.disabled = !gameActive || isProcessing || isSpeaking;
        stopGameBtn.disabled = !gameActive || isProcessing; settingsBtn.disabled = isProcessing; showContribsBtn.disabled = isProcessing;
        startRecordBtn.disabled = !canRecord || (mediaRecorder && mediaRecorder.state === "recording");
        stopRecordBtn.disabled = !gameActive || !(mediaRecorder && mediaRecorder.state === "recording"); stopTTSBtn.disabled = !isSpeaking;
    }
    function setProcessingStatus(processing) { /* ... (API countdown logic as before) ... */
        isProcessing = processing; loadingIndicator.style.display = processing ? "block" : "none"; apiCountdownSpan.textContent = "";
        if (processing) { let count = ESTIMATED_API_TIME; apiCountdownSpan.textContent = ` (approx. ${count}s)`; if (apiCountdownInterval) clearInterval(apiCountdownInterval); apiCountdownInterval = setInterval(() => { count--; apiCountdownSpan.textContent = ` (approx. ${count}s)`; if (count <= 0) { clearInterval(apiCountdownInterval); apiCountdownSpan.textContent = ""; }}, 1000); }
        else { if (apiCountdownInterval) clearInterval(apiCountdownInterval); apiCountdownInterval = null; apiCountdownSpan.textContent = ""; } updateUIState();
    }
    function setSpeakingStatus(speaking) { /* ... (Handling isSpeaking flag and buttons as before) ... */
        isSpeaking = speaking; if (!speaking) { if (currentAudioElement) { currentAudioElement.pause(); currentAudioElement = null; } window.speechSynthesis.cancel(); } updateUIState();
    }
    function updateModelInfo() {
        let ttsInfo = "Browser SpeechSynthesis";
        if (currentTtsProvider === 'elevenlabs' && elevenLabsKeyInput.value.trim()) {
            ttsInfo = "ElevenLabs TTS";
        } else if (currentTtsProvider === 'google' && googleApiKeyInput.value.trim() && googleVoiceNameInput.value.trim()) {
            ttsInfo = "Google Cloud TTS";
        }
        let sttInfo = useBrowserSTT ? "Browser SpeechRecognition" : (apiKey ? currentSTTModel : "No API Key");
        modelInfoDiv.innerHTML = `Using models: STT: ${sttInfo} | TTS: ${ttsInfo}`;
        modelInfoSettingsDiv.innerHTML = modelInfoDiv.innerHTML;
    }
    function updateHelpInfosVisibility() { /* ... (Visibility logic as before) ... */
        const show = showHelpInfosCheckbox.checked; storyDebugDiv.style.display = show ? "block" : "none"; hintDebugDiv.style.display = show ? "block" : "none"; if(show) updateHelpInfosContent();
    }
    function updateHelpInfosContent() { /* ... (Content update logic as before) ... */
        if (showHelpInfosCheckbox.checked) { storyDebugDiv.innerText = `Last Reaction Req:\n${lastReactionRequest||'N/A'}\n\nLast Reaction Res:\n${lastReactionResponse||'N/A'}`; hintDebugDiv.innerText = `Last Start/Hint Req:\n${lastStartRequest||lastReactionRequest||'N/A'}\n\nLast Start/Hint Res:\n${lastStartResponse||lastReactionResponse||'N/A'}`; }
    }
    function safeParseJSON(text) { /* ... (Parsing logic as before) ... */
        try { return JSON.parse(text); } catch (e) { debugLog(`Std JSON parse failed: ${e}. Extracting...`); const s=text.indexOf('{'), end=text.lastIndexOf('}'); if (s !== -1 && end !== -1 && end > s) { const json = text.substring(s, end + 1); try { const p = JSON.parse(json); debugLog("JSON extracted ok."); return p; } catch (e2) { debugLog(`Extraction parse error: ${e2}.`); return null; }} else { debugLog(`No JSON structure found.`); return null; } }
    }
    function removeDuplicatePhrases(text) { /* ... (Logic as before) ... */
        const l=text.length; if (l > 10 && text.substring(0, l/2).trim() === text.substring(l/2).trim()) { debugLog("Shortened repeat transcript."); return text.substring(0, l/2).trim(); } return text;
    }
    function debugLog(message) { const ts = new Date().toISOString(); const msg = `[${ts}] ${message}`; debugLogDiv.innerHTML = `${msg}<br>` + debugLogDiv.innerHTML; console.log(msg); }
    function openModal(id) { document.getElementById(id).style.display = "block"; debugLog(`Modal ${id} opened.`); }
    function closeModal(id) { document.getElementById(id).style.display = "none"; debugLog(`Modal ${id} closed.`); }

    // --- Game Flow Functions ---
    async function startGame() { /* ... (Start game logic as before) ... */
        if (!apiKey) { alert("Please enter API Key in settings."); openModal("settingsModal"); return; }
        if (!navigator.mediaDevices?.getUserMedia) { alert("MediaDevices API not supported."); return; }
        setProcessingStatus(true);
        try {
            gameActive = true; fullStory = ""; currentHint = ""; lastComment = ""; lastUserInput = ""; userContributions = [];
            lastStartRequest = ""; lastStartResponse = ""; lastReactionRequest = ""; lastReactionResponse = "";
            recordButtonsDiv.style.display = "block"; updateUIState();
            lastUserTextDiv.textContent = "Waiting for first contribution..."; ironicTextDiv.textContent = "Waiting for first contribution...";
            debugLog("Starting game...");
            const initialData = await fetchBotStart(); setProcessingStatus(false);
            if (initialData) {
                fullStory = initialData.startSentence || "Error."; currentHint = initialData.hint || "Error.";
                storyTextDiv.textContent = fullStory; hintTextDiv.textContent = currentHint;
                updateHelpInfosContent(); debugLog("Initial story loaded.");
                setSpeakingStatus(true);
                try { await speakText(fullStory, currentLanguage); if (isSpeaking) await new Promise(r => setTimeout(r, 300)); if (isSpeaking) await speakText(currentHint, currentLanguage); }
                catch (err) { if (err !== "Cancelled") debugLog(`TTS startup err: ${err}`); } finally { setSpeakingStatus(false); }
                debugLog("Game started successfully.");
            } else { storyTextDiv.textContent = "Error starting game."; hintTextDiv.textContent = ""; gameActive = false; recordButtonsDiv.style.display = "none"; updateUIState(); }
        } catch(error) { debugLog(`startGame Err: ${error}`); alert("Error starting."); gameActive = false; recordButtonsDiv.style.display = "none"; setProcessingStatus(false); updateUIState(); }
    }
    function restartGame() { debugLog("Restarting game..."); stopGame(); setTimeout(startGame, 500); }
    function stopGame() { /* ... (Stop game logic as before) ... */
        gameActive = false; stopRecording(); stopSpeaking(); if (apiCountdownInterval) clearInterval(apiCountdownInterval);
        recordButtonsDiv.style.display = "none"; recordCountdownDiv.textContent = ""; setProcessingStatus(false);
        fullStory = ""; currentHint = ""; lastComment = ""; lastUserInput = ""; userContributions = [];
        debugLog("Game stopped."); updateUIState();
    }

    // --- Recording Functions ---
    async function recordUserSpeech() {
        if (!gameActive) { debugLog("Record attempt while inactive."); return; }
        
        // Use browser's speech recognition if enabled or no API key available
        if (useBrowserSTT || !apiKey) {
            recordWithBrowserSTT();
            return;
        }
        
        // Otherwise use the original recording logic with OpenAI API
        setProcessingStatus(true); 
        let stream; 
        try { 
            stream = await navigator.mediaDevices.getUserMedia({ audio: true }); 
            debugLog("Mic ok."); 
        } catch (err) { 
            alert("Mic access denied."); 
            debugLog(`Mic err: ${err}`); 
            setProcessingStatus(false); 
            return; 
        } 
        setProcessingStatus(false);
        
        audioChunks = []; 
        const mTypes = ['audio/webm;codecs=opus', 'audio/ogg;codecs=opus', 'audio/wav']; 
        const supType = mTypes.find(t => MediaRecorder.isTypeSupported(t)); 
        debugLog(`Mime: ${supType || 'Default'}`);
        
        try { 
            mediaRecorder = new MediaRecorder(stream, { mimeType: supType || undefined }); 
        } catch (e) { 
            debugLog(`Recorder err: ${e}`); 
            alert("Recording init err."); 
            return; 
        }
        
        mediaRecorder.ondataavailable = e => { 
            if (e.data.size > 0) audioChunks.push(e.data); 
        };
        
        mediaRecorder.onstop = async () => {
            setProcessingStatus(true);
            try {
                if (recordCountdownInterval) clearInterval(recordCountdownInterval); 
                recordCountdownDiv.textContent = ""; 
                stream.getTracks().forEach(t => t.stop()); 
                debugLog("Recording stopped.");
                
                if (audioChunks.length === 0) { 
                    debugLog("No audio."); 
                    return; 
                }
                
                const blob = new Blob(audioChunks, { type: supType || 'audio/wav' }); 
                const fName = `userSpeech.${(supType || 'wav').split('/')[1].split(';')[0] || 'wav'}`;
                debugLog(`Blob (${(blob.size/1024).toFixed(1)} KB). Transcribing...`); 
                audioChunks = [];
                
                let transcript = await sendAudioToTranscribe(blob, fName);
                if (transcript && transcript.trim()) { 
                    transcript = removeDuplicatePhrases(transcript.trim()); 
                    lastUserInput = transcript; 
                    lastUserTextDiv.textContent = transcript; 
                    debugLog("Transcript: " + transcript); 
                    await processUserInput(transcript); 
                } else { 
                    debugLog("Empty transcript."); 
                    lastUserInput = ""; 
                    lastUserTextDiv.textContent = "(Empty input)"; 
                    ironicTextDiv.textContent = "Couldn't understand."; 
                    setProcessingStatus(false); 
                    setSpeakingStatus(true); 
                    try { 
                        await speakText("Sorry, I couldn't understand.", currentLanguage); 
                    } catch (err) { 
                        if (err !== "Cancelled") debugLog(`TTS Err: ${err}`); 
                    } finally { 
                        setSpeakingStatus(false); 
                    } 
                }
            } catch (err) { 
                debugLog(`Record stop/process err: ${err}`); 
                alert("Error processing recording."); 
            } finally { 
                setProcessingStatus(false); 
            }
        };
        
        mediaRecorder.start(); 
        debugLog("Recording started."); 
        updateUIState(); 
        let rem = RECORD_TIME; 
        recordCountdownDiv.textContent = `Recording... (${rem}s)`; 
        if (recordCountdownInterval) clearInterval(recordCountdownInterval); 
        recordCountdownInterval = setInterval(() => { 
            rem--; 
            recordCountdownDiv.textContent = `Recording... (${rem}s)`; 
            if (rem <= 0) { 
                debugLog("Max time."); 
                stopRecording(); 
            }
        }, 1000);
    }
    
    // Function to record using browser's Speech Recognition API
    function recordWithBrowserSTT() {
        if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
            alert("Your browser doesn't support speech recognition. Try using Chrome, Edge, or Safari.");
            debugLog("Browser speech recognition not supported.");
            return;
        }
        
        try {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognitionInstance = new SpeechRecognition();
            
            // Configure the recognition
            recognitionInstance.lang = googleLangCodeMap[currentLanguage] || 'en-US';
            recognitionInstance.continuous = false;
            recognitionInstance.interimResults = false;
            recognitionInstance.maxAlternatives = 1;
            
            // Update UI
            setProcessingStatus(true);
            recordCountdownDiv.textContent = "Listening... (click Stop when finished)";
            debugLog("Browser STT: Started listening...");
            updateUIState();
            
            // Set up event listeners
            recognitionInstance.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                debugLog(`Browser STT result: ${transcript}`);
                
                if (transcript && transcript.trim()) {
                    const cleanedTranscript = removeDuplicatePhrases(transcript.trim());
                    lastUserInput = cleanedTranscript;
                    lastUserTextDiv.textContent = cleanedTranscript;
                    await processUserInput(cleanedTranscript);
                } else {
                    debugLog("Empty transcript from browser STT.");
                    lastUserInput = "";
                    lastUserTextDiv.textContent = "(Empty input)";
                    ironicTextDiv.textContent = "Couldn't understand.";
                    setProcessingStatus(false);
                    setSpeakingStatus(true);
                    try {
                        await speakText("Sorry, I couldn't understand.", currentLanguage);
                    } catch (err) {
                        if (err !== "Cancelled") debugLog(`TTS Err: ${err}`);
                    } finally {
                        setSpeakingStatus(false);
                    }
                }
            };
            
            recognitionInstance.onerror = (event) => {
                debugLog(`Browser STT error: ${event.error}`);
                setProcessingStatus(false);
                recordCountdownDiv.textContent = "";
                updateUIState();
                
                if (event.error === 'no-speech') {
                    alert("No speech detected. Please try again.");
                } else if (event.error !== 'aborted') {
                    alert(`Speech recognition error: ${event.error}`);
                }
            };
            
            recognitionInstance.onend = () => {
                debugLog("Browser STT: Recognition ended.");
                setProcessingStatus(false);
                recordCountdownDiv.textContent = "";
                updateUIState();
                recognitionInstance = null;
            };
            
            // Start recognition
            recognitionInstance.start();
            
        } catch (error) {
            debugLog(`Browser STT init error: ${error}`);
            alert("Error initializing speech recognition. Try again or check browser compatibility.");
            setProcessingStatus(false);
            recordCountdownDiv.textContent = "";
            updateUIState();
        }
    }
    function stopRecording() {
        // If using browser STT
        if (recognitionInstance) {
            recognitionInstance.stop();
            debugLog("Browser STT: stopping recognition");
            return;
        }
        
        // Regular MediaRecorder logic
        if (mediaRecorder?.state === "recording") { 
            mediaRecorder.stop(); 
            debugLog("stopRecording called."); 
        } else { 
            debugLog("stopRecording: recorder inactive."); 
            if (recordCountdownInterval) clearInterval(recordCountdownInterval); 
            recordCountdownDiv.textContent = ""; 
            updateUIState(); 
        }
    }

    // --- TTS Functions ---
    function stopSpeaking() { /* ... (Stop speaking logic as before) ... */
         debugLog("Stopping speech..."); window.speechSynthesis.cancel(); if (currentAudioElement) { currentAudioElement.pause(); currentAudioElement.src = ""; currentAudioElement = null; debugLog("ElevenLabs stopped."); } setSpeakingStatus(false);
    }

    // Helper to convert Base64 to Blob
    async function base64ToBlob(base64, contentType = 'audio/mp3') {
        try {
            const byteCharacters = atob(base64);
            const byteNumbers = new Array(byteCharacters.length);
            for (let i = 0; i < byteCharacters.length; i++) {
                byteNumbers[i] = byteCharacters.charCodeAt(i);
            }
            const byteArray = new Uint8Array(byteNumbers);
            return new Blob([byteArray], {type: contentType});
        } catch (e) {
            debugLog("Error decoding Base64: " + e);
            return null;
        }
    }


    async function speakText(text, lang) {
        if (!text || !isSpeaking) { debugLog("speakText skipped."); return Promise.resolve(); }
        const targetLang = lang || currentLanguage;
        debugLog(`speakText executing (Lang: ${targetLang}, Provider: ${currentTtsProvider}). Text: "${text.substring(0, 50)}..."`);
        const ttsStyle = ttsStyleInput.value || "A clear, neutral voice."; // Style might have minimal effect

        // --- Provider Logic ---
        if (currentTtsProvider === 'google') {
            const googleKey = googleApiKeyInput.value.trim();
            const googleVoice = googleVoiceNameInput.value.trim();
            if (googleKey && googleVoice) {
                currentTTSModel = "Google Cloud TTS"; updateModelInfo();
                debugLog(`Attempting speech with Google TTS (Voice: ${googleVoice}, Lang: ${targetLang}).`);
                try {
                    const googleLangCode = googleLangCodeMap[targetLang] || 'en-US'; // Map to Google codes
                    const requestBody = {
                        input: { text: text },
                        voice: { languageCode: googleLangCode, name: googleVoice },
                        audioConfig: { audioEncoding: "MP3", speakingRate: 1.0 } // Adjust speakingRate if desired
                    };
                    // Use query parameter for API key (simpler for client-side, **less secure**)
                    const response = await fetch(`https://texttospeech.googleapis.com/v1/text:synthesize?key=${googleKey}`, {
                        method: "POST",
                        headers: { "Content-Type": "application/json" },
                        body: JSON.stringify(requestBody)
                    });
                    if (!response.ok) { let eD=null; try{eD=await response.json();}catch(e){} throw new Error(`Google TTS API Error: ${eD?.error?.message || response.status}`); }
                    const data = await response.json();
                    if (data.audioContent) {
                        const audioBlob = await base64ToBlob(data.audioContent, 'audio/mp3');
                        if (!audioBlob) throw new Error("Failed to decode Google audio content.");
                        const audioUrl = URL.createObjectURL(audioBlob);
                        if (!isSpeaking) { URL.revokeObjectURL(audioUrl); return Promise.reject("Cancelled"); }
                        currentAudioElement = new Audio(audioUrl); debugLog("Google TTS audio playing...");
                        await new Promise((resolve, reject) => {
                             const audio = currentAudioElement; if (!audio) return reject("Audio null");
                             const onEnd = () => { if(currentAudioElement === audio){debugLog("Google TTS finished.");currentAudioElement=null;URL.revokeObjectURL(audioUrl);resolve();} else reject("Stopped externally"); };
                             const onError = (e)=>{ if(currentAudioElement === audio){debugLog(`Google TTS Playback Err: ${e}`);currentAudioElement=null;URL.revokeObjectURL(audioUrl);reject(e);} else reject("Stopped externally"); };
                             audio.addEventListener('ended', onEnd, {once:true}); audio.addEventListener('error', onError, {once:true});
                             if (!isSpeaking) { URL.revokeObjectURL(audioUrl); currentAudioElement = null; return reject("Cancelled"); }
                             audio.play().catch(err => { if(currentAudioElement === audio){debugLog(`Play start err: ${err}`);currentAudioElement = null;URL.revokeObjectURL(audioUrl);reject(err);} });
                         }); return Promise.resolve(); // Success with Google
                    } else { throw new Error("Google TTS API response missing audioContent."); }
                } catch (error) { debugLog(`Google TTS Error: ${error}. Falling back.`); currentAudioElement = null; if (!isSpeaking) return Promise.reject("Cancelled"); /* Fallback below */ }
            } else { debugLog("Google TTS selected but Key or Voice missing. Falling back."); /* Fallback below */ }
        }
        else if (currentTtsProvider === 'elevenlabs') {
            const elevenLabsKey = elevenLabsKeyInput.value.trim();
            const elevenLabsVoice = elevenLabsVoiceInput.value.trim() || "TxGEqnHWrfWFTfGW9XjX";
            if (elevenLabsKey) {
                currentTTSModel = "ElevenLabs TTS"; updateModelInfo();
                debugLog(`Attempting speech with ElevenLabs (Voice: ${elevenLabsVoice}).`);
                try { // Try ElevenLabs
                    const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${elevenLabsVoice}`, { method: "POST", headers: { "Accept": "audio/mpeg", "xi-api-key": elevenLabsKey, "Content-Type": "application/json" }, body: JSON.stringify({ text: text, model_id: "eleven_multilingual_v2", voice_settings: { stability: 0.6, similarity_boost: 0.8 } }) });
                    if (!response.ok) { let eD=null; try{eD=await response.json();}catch(e){} throw new Error(`ElevenLabs API Error: ${eD?.detail?.message || response.status}`); }
                    const audioBlob = await response.blob(); const audioUrl = URL.createObjectURL(audioBlob);
                    if (!isSpeaking) { URL.revokeObjectURL(audioUrl); return Promise.reject("Cancelled"); }
                    currentAudioElement = new Audio(audioUrl); debugLog("ElevenLabs playing...");
                    await new Promise((resolve, reject) => { /* ... (promise logic as before) ... */
                         const audio = currentAudioElement; if (!audio) return reject("Audio null");
                         const onEnd = () => { if (currentAudioElement === audio) { debugLog("ElevenLabs ended."); currentAudioElement = null; URL.revokeObjectURL(audioUrl); resolve(); } else reject("Stopped externally");};
                         const onError = (e) => { if (currentAudioElement === audio) { debugLog(`ElevenLabs Error: ${e}`); currentAudioElement = null; URL.revokeObjectURL(audioUrl); reject(e); } else reject("Stopped externally");};
                         audio.addEventListener('ended', onEnd, { once: true }); audio.addEventListener('error', onError, { once: true });
                         if (!isSpeaking) { URL.revokeObjectURL(audioUrl); currentAudioElement = null; return reject("Cancelled"); }
                         audio.play().catch(err => { if (currentAudioElement === audio) { debugLog(`Play start err: ${err}`); currentAudioElement = null; URL.revokeObjectURL(audioUrl); reject(err); }});
                     }); return Promise.resolve(); // Success with ElevenLabs
                } catch (error) { debugLog(`ElevenLabs TTS Error: ${error}. Falling back.`); currentAudioElement = null; if (!isSpeaking) return Promise.reject("Cancelled"); /* Fallback below */ }
            } else { debugLog("ElevenLabs selected but Key missing. Falling back."); /* Fallback below */ }
        }

        // Fallback to Browser TTS (if provider is 'browser' or other providers failed/misconfigured)
        try { await fallbackTTS(text, targetLang); return Promise.resolve(); }
        catch (error) { debugLog(`Browser TTS Error: ${error}`); return Promise.reject(error === "Cancelled" ? "Cancelled" : error); }
    }

    function fallbackTTS(text, lang) { /* ... (Fallback logic as before) ... */
        return new Promise(async (resolve, reject) => {
            if (!isSpeaking) { debugLog("fallbackTTS cancelled."); return reject("Cancelled"); }
            currentTTSModel = "Browser TTS"; updateModelInfo(); debugLog(`Using Browser TTS (Lang: ${lang}).`);
            const utterance = new SpeechSynthesisUtterance(text); const langCode = lang || currentLanguage; const browserTag = googleLangCodeMap[langCode] || 'en-US'; utterance.lang = browserTag; // Use Google codes map for consistency
            let voices = window.speechSynthesis.getVoices(); if (voices.length === 0) { await new Promise(res => window.speechSynthesis.onvoiceschanged = res); voices = window.speechSynthesis.getVoices(); }
            let voice = voices.find(v => v.lang === browserTag) || voices.find(v => v.lang.startsWith(langCode)); if (voice) { utterance.voice = voice; debugLog(`Voice found: ${voice.name}`); } else { debugLog(`No voice for ${browserTag}.`); }
            utterance.onend = () => { debugLog("Browser TTS finished."); resolve(); }; utterance.onerror = (e) => { debugLog(`Browser TTS Error: ${e.error}`); reject(e.error); };
            if (!isSpeaking) { debugLog("Browser TTS cancelled before speak."); return reject("Cancelled"); } window.speechSynthesis.speak(utterance);
        });
    }

    // --- API Call Functions ---
    async function sendAudioToTranscribe(audioBlob, fileName) {
        if (!apiKey) {
            throw new Error("API Key missing. Consider enabling browser speech recognition.");
        }
        
        const fD = new FormData();
        fD.append("file", audioBlob, fileName);
        fD.append("model", currentSTTModel);
        fD.append("language", currentLanguage);
        fD.append("response_format", "json");
        debugLog(`Transcribing ${fileName} (Lang: ${currentLanguage})...`);
        
        try {
            const r = await fetch("https://api.openai.com/v1/audio/transcriptions", {
                method: "POST",
                headers: { "Authorization": "Bearer " + apiKey },
                body: fD
            });
            
            const d = await r.json();
            if (!r.ok) throw new Error(`Transcription API Err: ${d?.error?.message || r.status}`);
            
            debugLog("Transcript OK.");
            return d.text;
        } catch (e) {
            debugLog(`Transcript Err: ${e}`);
            throw e;
        }
    }
    async function fetchBotStart() { /* ... (Start logic as before, using languageNameMap) ... */
        let p = initialPromptInput.value; const lN = languageNameMap[currentLanguage] || 'English'; p += `\nGenerate the response in ${lN}.`; const t = themeSelect.value; if (t && t !== "standard" && t !== 'german-a2') p += `\nConsider the theme: ${t}.`; debugLog("fetchBotStart Prompt:\n" + p); lastStartRequest = p; lastStartResponse = "";
        try { const r = await fetch("https://api.openai.com/v1/chat/completions", { method: "POST", headers: { "Content-Type": "application/json", "Authorization": "Bearer " + apiKey }, body: JSON.stringify({ model: "gpt-4-turbo", messages: [{ role: "user", content: p }], max_tokens: 400, temperature: 0.8, response_format: { type: "json_object" } }) }); const d = await r.json(); debugLog("fetchBotStart Raw: " + JSON.stringify(d)); if (!r.ok) throw new Error(`API Err: ${d?.error?.message || r.status}`); lastStartResponse = d.choices?.[0]?.message?.content || ""; const parsed = safeParseJSON(lastStartResponse); if (parsed?.startSentence && parsed?.hint) { debugLog("fetchBotStart OK."); updateHelpInfosContent(); return parsed; } else { throw new Error("JSON Parse/Keys err."); } } catch (err) { debugLog(`fetchBotStart Err: ${err}`); lastStartResponse = `${err}`; updateHelpInfosContent(); return null; }
    }
    async function fetchBotReaction(storySoFar, userInput) { /* ... (Reaction logic updated for themes as before) ... */
        let baseP = reactionPromptInput.value.replace("{story}", storySoFar || "[No story]").replace("{lastUser}", userInput || "[No input]"); let finalP = baseP; const theme = themeSelect.value; const commentLang = languageNameMap[currentLanguage] || 'English'; let storyHintLangDesc = commentLang; let themeInstructions = "";
        if (theme === 'german-a2') { storyHintLangDesc = "simple German (A2 level)"; finalP += `\nGenerate \`integratedStory\` and \`hint\` in **${storyHintLangDesc}**.`; finalP += `\nGenerate \`comment\` in **${commentLang}**.`; themeInstructions = germanA2Prompt.value.replace("{lastUser}", userInput || "[No input]").replace(/{commentLang}/g, commentLang); debugLog(`Using A2 prompt (Story: ${storyHintLangDesc}, Comment: ${commentLang}).`); }
        else { finalP += `\nGenerate response (story, comment, hint) in **${commentLang}**.`; switch(theme) { case 'lifecoach': themeInstructions = lifecoachPrompt.value.replace(/{commentLang}/g, commentLang); break; case 'creative-writing': themeInstructions = creativewritingPrompt.value.replace(/{commentLang}/g, commentLang); break; case 'mindfulness': themeInstructions = mindfulnessPrompt.value.replace(/{commentLang}/g, commentLang); break; case 'historical-whatif': themeInstructions = historicalwhatifPrompt.value.replace(/{commentLang}/g, commentLang); break; } if (themeInstructions) { debugLog(`Using theme prompt: ${theme}`); } else if (theme && theme !== "standard") { finalP += `\nConsider theme: ${theme}.`; } }
        if(themeInstructions) finalP += "\n" + themeInstructions;
        debugLog("fetchBotReaction Prompt:\n" + finalP); lastReactionRequest = finalP; lastReactionResponse = "";
        try { const r = await fetch("https://api.openai.com/v1/chat/completions", { method: "POST", headers: { "Content-Type": "application/json", "Authorization": "Bearer " + apiKey }, body: JSON.stringify({ model: "gpt-4-turbo", messages: [{ role: "user", content: finalP }], max_tokens: 1500, temperature: 0.9, response_format: { type: "json_object" } }) }); const d = await r.json(); debugLog("fetchBotReaction Raw: " + JSON.stringify(d)); if (!r.ok) { const msg = d?.error?.message||r.status; if (msg.includes("context_length")) alert("Story too long."); throw new Error(`API Err: ${msg}`); } const finish = d.choices?.[0]?.finish_reason; lastReactionResponse = d.choices?.[0]?.message?.content || ""; if (finish === "length") { debugLog("WARN: Finish 'length'."); lastReactionResponse += "\n[TRUNCATED!]"; } if (lastReactionResponse) { const p = safeParseJSON(lastReactionResponse); if (p?.integratedStory && p?.comment && p?.hint) { debugLog("fetchBotReaction OK."); updateHelpInfosContent(); return p; } else { throw new Error("JSON Parse/Keys err."); } } else { throw new Error("Empty response."); }
        } catch (err) { debugLog(`fetchBotReaction Err: ${err}`); lastReactionResponse = `${err}`; updateHelpInfosContent(); return null; }
    }
    async function processUserInput(transcript) { /* ... (Process user input logic as before, catching "Cancelled") ... */
       if (!gameActive) { debugLog("processUserInput skipped: inactive."); return; }
       try {
           const botData = await fetchBotReaction(fullStory, transcript); // API call
           if (botData) {
               fullStory = botData.integratedStory; currentHint = botData.hint; lastComment = botData.comment;
               storyTextDiv.textContent = fullStory; ironicTextDiv.textContent = lastComment; hintTextDiv.textContent = currentHint;
               updateHelpInfosContent(); debugLog("Bot reaction processed.");
               setSpeakingStatus(true);
               try { // Wrap speaking sequence
                   const storyLang = (themeSelect.value === 'german-a2') ? 'de' : currentLanguage;
                   await speakText(lastComment, currentLanguage); // Comment in UI lang
                   if(isSpeaking) await new Promise(r => setTimeout(r, 300));
                   if(isSpeaking) await speakText(fullStory, storyLang); // Story/Hint in specific lang
                   if(isSpeaking) await new Promise(r => setTimeout(r, 300));
                   if(isSpeaking) await speakText(currentHint, storyLang);
               } catch(ttsError) { // Catch errors *or* cancellation
                   if(ttsError !== "Cancelled") { debugLog(`TTS sequence error: ${ttsError}`); alert("Speech error."); }
                   else { debugLog("TTS sequence cancelled by user."); }
               } finally { setSpeakingStatus(false); } // Always reset speaking status
               debugLog("User input processing & TTS sequence finished.");
           } else { ironicTextDiv.textContent = "Error processing response."; hintTextDiv.textContent = ""; debugLog("Processing failed: botData null."); }
       } catch(error) { debugLog(`processUserInput Err: ${error}`); alert("Error processing input."); ironicTextDiv.textContent = "Error."; hintTextDiv.textContent = ""; setSpeakingStatus(false); }
     }

    // --- UI Helper Functions ---
    function updateContribsModal() { /* ... (Modal update logic as before) ... */
         contribList.innerHTML = ""; if (userContributions.length === 0) { contribList.innerHTML = "<li>No contributions yet.</li>"; } else { userContributions.forEach((c, i) => { const li = document.createElement("li"); li.textContent = `${i + 1}. ${c}`; contribList.appendChild(li); }); }
    }

    // --- Auto-Start ---
    setInterval(() => { /* ... (Auto-start logic as before) ... */
        const now = new Date(); const [h, m] = autoStartTimeInput.value.split(":").map(Number); if (!isNaN(h) && !isNaN(m) && now.getHours()===h && now.getMinutes()===m && !gameActive && !isProcessing && !isSpeaking) { debugLog(`Auto-Start Time (${autoStartTimeInput.value}) reached.`); startGame(); }
    }, 60000);

  </script>
</body>
</html>
